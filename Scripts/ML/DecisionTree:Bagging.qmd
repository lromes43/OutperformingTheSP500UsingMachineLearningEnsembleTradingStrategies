---
title: "Untitled"
format: html
---


```{python}
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn import preprocessing
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import dtreeviz
from datetime import date
from plotnine import *
import pickle
import joblib
```


```{python}

train = pd.read_feather("/Users/lukeromes/Desktop/Personal/Sp500Project/Data/TrainData.feather")
test = pd.read_feather("/Users/lukeromes/Desktop/Personal/Sp500Project/Data/TestData.feather")
```


```{python}
cols_to_drop = ['Date', 'Day_to_Day_Diff', 'Day_to_Day_Percent_Diff', 'Five_Day_Percent_Diff', 'Two_Week_Percent_Diff', 'Monthly_Percent_Diff', 'Daily_Return', 'Day_Range']


X_train = train.drop(columns=cols_to_drop + ['Movement'])
y_train = train['Movement']

X_test = test.drop(columns=cols_to_drop + ['Movement'])
y_test = test['Movement']

X_train_final = pd.get_dummies(X_train, drop_first=True)
X_test_final = pd.get_dummies(X_test, drop_first = True)
X_test = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)

```

Creating Tree


```{python}
tree = DecisionTreeClassifier(random_state=123, max_depth=4)
tree.fit(X_train_final, y_train_enc)
```


```{python}
y_pred_enc = tree.predict(X_test)
y_pred_final = le.inverse_transform(y_pred_enc)
accuracy = accuracy_score(y_test, y_pred_final)

```

Confusion Matrix 


```{python}
cm = confusion_matrix(y_test, y_pred_final)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= le.classes_)
disp.plot(cmap = "Blues")
plt.show()
```


Bagging


```{python}
bagged_model = BaggingClassifier(
    n_estimators=300,
    max_samples=1.0,
    max_features=1.0,
    bootstrap=True,
    bootstrap_features=False,
    oob_score=True,
    n_jobs=-1,
    random_state=123

)

bagged_model.fit(X_train_final, y_train_enc)
```

```{python}
oob_score = bagged_model.oob_score_
ypred_enc = bagged_model.predict(X_test_final)
ypred = le.inverse_transform(ypred_enc)
bagged_accuracy = accuracy_score(y_test, ypred)
```

Confusion Matrix 

```{python}
cm = confusion_matrix(y_test, ypred_enc)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(cmap = "Blues")
plt.show()

```

Saving the models

```{python}
# Save the Decision Tree model
filename_tree = 'tree.pkl'
with open(filename_tree, 'wb') as file:
    pickle.dump(tree, file)

# Save the Bagged Model
import joblib
file_name = 'bagged_ensemble_model.pkl'
joblib.dump(bagged_model, file_name)
```

Determining Optimal Number Iterations

```{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

from tqdm import tqdm

n_trees = range(10, 1000, 100)
oob_errors = []

for n in tqdm(n_trees, desc="Training Bagging Models"):
    bagged_model2 = BaggingClassifier(
        estimator=DecisionTreeClassifier(random_state=123),
        n_estimators=n,
        oob_score=True,
        n_jobs=-1,
        random_state=123
    )

    bagged_model2.fit(X_train_final, y_train_enc)
    if hasattr(bagged_model2, 'oob_score_'):
        oob_error = 1 - bagged_model2.oob_score_
        oob_errors.append((n, oob_error))
    else:
        print(f"OOB score not available for n_estimators={n}")

print("OOB Errors calculated successfully.")

```


```{python}
ypred_enc = bagged_model2.predict(X_test_final)
ypred = le.inverse_transform(ypred_enc)
bagged2_accuracy = accuracy_score(y_test, ypred)
```

```{python}
oob_df = pd.DataFrame(oob_errors, columns=['n_trees', 'oob_error'])
min_error_index = oob_df['oob_error'].idxmin()
min_error = float(oob_df.loc[min_error_index, 'oob_error'])
best_num_trees =int(oob_df.loc[min_error_index, 'n_trees'])
```


```{python}
oob_plot = (ggplot(aes(x = 'n_trees',
                       y = 'oob_error'))+
                       ylim(0.3,0.315) +
                    geom_point(color = 'red')+
                    geom_smooth(color = 'blue'))  


oob_plot
```

```{python}
min_error = 0.311
best_number_trees = 710


final_bagged = BaggingClassifier(
     estimator=DecisionTreeClassifier(random_state=123),
     n_estimators=best_number_trees,
     oob_score=True,
     n_jobs=-1,
     random_state=123
    )
final_bagged.fit(X_train_final, y_train_enc)
```

```{python}
optimal_iterations_yhat_enc = final_bagged.predict(X_test_final)
optimal_iterations_yhat = le.inverse_transform(optimal_iterations_yhat_enc)
optimal_iterations_acc = accuracy_score(y_test, optimal_iterations_yhat )
cm = confusion_matrix(y_test, optimal_iterations_yhat )
optimal_uterations_plot = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
optimal_uterations_plot.plot(cmap = "Blues")
plt.title("Confusion Matrix for Optimal Iterations")
plt.show()

```
Getting 53.9% accuracy

```{python}
bagged_optimal_iterations = final_bagged
final_model_filename = 'bagged_optimal_iterations.joblib'
joblib.dump(bagged_optimal_iterations , final_model_filename)
```


Variable Importance

```{python}
n_features = X_train_final.shape[1]
global_importance = np.zeros(n_features, dtype=float)

has_feature_subsampling = hasattr(final_bagged, "estimators_features_") and final_bagged.estimators_features_ is not None

num_used_estimators = 0
for i, est in enumerate(final_bagged.estimators_):
    if not hasattr(est, "feature_importances_"):
        continue

    fi = est.feature_importances_ 
    if has_feature_subsampling:
        feats_idx = final_bagged.estimators_features_[i]
        tmp = np.zeros(n_features, dtype=float)
        tmp[feats_idx] = fi
        global_importance += tmp
    else:
       
        global_importance += fi

    num_used_estimators += 1 


if num_used_estimators > 0:
    global_importance /= num_used_estimators
else:
    raise RuntimeError("No usable base estimators with feature_importances_.")


imp_df = (
    pd.DataFrame({
        "feature": X_train_final.columns,
        "importance": global_importance
    })
    .sort_values("importance", ascending=False)
    .reset_index(drop=True)
)
```

```{python}
top_k = 10
imp_top = imp_df.head(top_k).copy() 


(
    ggplot(imp_top, aes(x="feature", y="importance"))
    + geom_col(fill= "blue")
    + coord_flip()
    + scale_y_continuous(expand=(0.0, 0.0))
    + labs(
        title=f"Variable Importance — Bagging (Top {top_k})",
        x="Feature",
        y="Mean Importance (across trees)"
    )
    + theme_minimal()
)
```

Can see that the ten most important features are:
VIX, RSI, OBV, MACD, Weekly_SD, Volume, Two_Week_SD, Monthly_SD, SD ( of middle bolinger band), Bollinger_Band, Lower

Final_Bagged_Tuning


```{python}

import pandas as pd
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from tqdm import tqdm 


param_grid = {
    "max_samples": [0.5, 0.7, 1.0],
    "max_features": [0.5, 0.7, 1.0],
}
results = []
total_iterations = len(param_grid['max_samples']) * len(param_grid['max_features'])


for n_samp in tqdm(param_grid['max_samples'], desc=f"Total Progress ({total_iterations} fits)"):
    for n_features in param_grid['max_features']:

        final_adjusted_bag = BaggingClassifier(
            estimator= DecisionTreeClassifier(random_state=123),
            n_estimators=best_number_trees, 
            max_samples= n_samp,
            max_features=n_features,
            bootstrap=True, 
            n_jobs=-1,
            random_state=123
        )
        

        cv_score = cross_val_score(
            final_adjusted_bag, 
            X_train_final, 
            y_train_enc, 
            cv=5, 
            scoring='accuracy', 
            n_jobs=-1
        )
        
        results.append({
                "max_samples": n_samp,
                "max_features": n_features,
                "mean_acc": np.mean(cv_score),
                "std_acc": np.std(cv_score)
            })
            
tune_df = pd.DataFrame(results)

print("\n--- Tuning Results ---")
print(tune_df)
```

Visualizing values from above

```{python}
tune_df = pd.read_csv("/Users/lukeromes/Desktop/Personal/Sp500Project/Scripts/ML/Bagged_tuned_results.csv")
tuned_results_plot = (ggplot(tune_df, aes(x = 'max_samples',
                                      y = 'mean_acc',
                                     color = 'factor(max_samples)',
                                     group = 'max_samples'))+
                                     geom_point()+
                                     geom_line()+
                                     facet_wrap('~max_features'))
tuned_results_plot
```

Optimal Values
```{python}
best_index = tune_df['mean_acc'].idxmax()
best_row = tune_df.loc[best_index]

best_estimator = best_row['n_samp']
best_max_samp = best_row['max_samples']
best_max_features = best_row['max_features']
best_cv_mean = best_row['mean_acc']
best_cv_std = best_row['std_acc']


print("Best parameters from CV:")
print(f"  n_estimators : {best_estimator}")
print(f"  max_samples  : {best_max_samples}")
print(f"  max_features : {best_max_features}")
print(f"  CV accuracy  : {best_cv_mean:.4f} ± {best_cv_std:.4f}")
```


Max samples = 1
max features = .5
cv acc = .6815 
n_estimators = 710

```{python}
best_estimator = 710
best_max_features = .5
bext_max_samp = 1
```

```{python}
tuned_bag = BaggingClassifier(
    estimator=DecisionTreeClassifier(random_state=123), 
    n_estimators=710, 
    max_samples=1,
    max_features=0.5, 
    bootstrap=True,
    n_jobs=-1, 
    random_state=123, 
    oob_score=True,   
)

tuned_bag.fit(X_train_final, y_train_enc)
```


```{python}
ypred_enc = FinalTunedBagged.predict(X_test_final)
ypred = le.inverse_transform(ypred_enc)
FinalTunedBagged_acc = accuracy_score(y_test, ypred)


cm = confusion_matrix(y_test, ypred)
FinalTunedBagged_cm = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=le.classes_)
FinalTunedBagged_cm.plot(cmap="Blues")
plt.title('FinalTunnedBagged')
plt.show()

```