---
title: "Untitled"
format: html
---


```{python}
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn import preprocessing
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import dtreeviz
from datetime import date
from plotnine import *
import pickle
import joblib
```


```{python}

train = pd.read_feather("/Users/lukeromes/Desktop/Personal/Sp500Project/Data/TrainData.feather")
test = pd.read_feather("/Users/lukeromes/Desktop/Personal/Sp500Project/Data/TestData.feather")
```


```{python}
cols_to_drop = ['Date', 'Day_to_Day_Diff', 'Day_to_Day_Percent_Diff', 'Five_Day_Percent_Diff', 'Two_Week_Percent_Diff', 'Monthly_Percent_Diff', 'Daily_Return', 'Day_Range']


X_train = train.drop(columns=cols_to_drop + ['Movement'])
y_train = train['Movement']

X_test = test.drop(columns=cols_to_drop + ['Movement'])
y_test = test['Movement']

X_train_final = pd.get_dummies(X_train, drop_first=True)
X_test_final = pd.get_dummies(X_test, drop_first = True)
X_test = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)

```

Creating Tree


```{python}
tree = DecisionTreeClassifier(random_state=123, max_depth=4)
tree.fit(X_train_final, y_train_enc)
```


```{python}
y_pred_enc = tree.predict(X_test)
y_pred_final = le.inverse_transform(y_pred_enc)
accuracy = accuracy_score(y_test, y_pred_final)

```

Confusion Matrix 


```{python}
cm = confusion_matrix(y_test, y_pred_final)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= le.classes_)
disp.plot(cmap = "Blues")
plt.show()
```


Bagging


```{python}
bagged_model = BaggingClassifier(
    n_estimators=300,
    max_samples=1.0,
    max_features=1.0,
    bootstrap=True,
    bootstrap_features=False,
    oob_score=True,
    n_jobs=-1,
    random_state=123

)

bagged_model.fit(X_train_final, y_train_enc)
```

```{python}
oob_score = bagged_model.oob_score_
ypred_enc = bagged_model.predict(X_test_final)
ypred = le.inverse_transform(ypred_enc)
bagged_accuracy = accuracy_score(y_test, ypred)
```

Confusion Matrix 

```{python}
cm = confusion_matrix(y_test, ypred_enc)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(cmap = "Blues")
plt.show()

```

Saving the models

```{python}
# Save the Decision Tree model
filename_tree = 'tree.pkl'
with open(filename_tree, 'wb') as file:
    pickle.dump(tree, file)

# Save the Bagged Model
import joblib
file_name = 'bagged_ensemble_model.pkl'
joblib.dump(bagged_model, file_name)
```

Determining Optimal Number Iterations

```{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

from tqdm import tqdm

n_trees = range(10, 1000, 100)
oob_errors = []

for n in tqdm(n_trees, desc="Training Bagging Models"):
    bagged_model2 = BaggingClassifier(
        estimator=DecisionTreeClassifier(random_state=123),
        n_estimators=n,
        oob_score=True,
        n_jobs=-1,
        random_state=123
    )

    bagged_model2.fit(X_train_final, y_train_enc)
    if hasattr(bagged_model2, 'oob_score_'):
        oob_error = 1 - bagged_model2.oob_score_
        oob_errors.append((n, oob_error))
    else:
        print(f"OOB score not available for n_estimators={n}")

print("OOB Errors calculated successfully.")

```


```{python}
ypred_enc = bagged_model2.predict(X_test_final)
ypred = le.inverse_transform(ypred_enc)
bagged2_accuracy = accuracy_score(y_test, ypred)
```

```{python}
oob_df = pd.DataFrame(oob_errors, columns=['n_trees', 'oob_error'])
min_error_index = oob_df['oob_error'].idxmin()
min_error = float(oob_df.loc[min_error_index, 'oob_error'])
best_num_trees =int(oob_df.loc[min_error_index, 'n_trees'])
```


```{python}
oob_plot = (ggplot(aes(x = 'n_trees',
                       y = 'oob_error'))+
                       ylim(0.3,0.315) +
                    geom_point(color = 'red')+
                    geom_smooth(color = 'blue'))  


oob_plot
```

```{python}
min_error = 0.311
best_number_trees = 710


final_bagged = BaggingClassifier(
     estimator=DecisionTreeClassifier(random_state=123),
     n_estimators=best_number_trees,
     oob_score=True,
     n_jobs=-1,
     random_state=123
    )
final_bagged.fit(X_train_final, y_train_enc)
```

```{python}
optimal_iterations_yhat_enc = final_bagged.predict(X_test_final)
optimal_iterations_yhat = le.inverse_transform(optimal_iterations_yhat_enc)
optimal_iterations_acc = accuracy_score(y_test, optimal_iterations_yhat )
cm = confusion_matrix(y_test, optimal_iterations_yhat )
optimal_uterations_plot = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
optimal_uterations_plot.plot(cmap = "Blues")
plt.title("Confusion Matrix for Optimal Iterations")
plt.show()

```
Getting 53.9% accuracy



```{python}
bagged_optimal_iterations = final_bagged
final_model_filename = 'bagged_optimal_iterations.joblib'
joblib.dump(bagged_optimal_iterations , final_model_filename)
```

```{python}

```

